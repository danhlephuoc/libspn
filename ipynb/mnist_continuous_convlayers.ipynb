{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative learning for continuous MNIST data using randomly structured SPNs\n",
    "This notebook shows how to build a randomly structured SPN and train it with online hard EM on continuous MNIST data.\n",
    "\n",
    "### Setting up the imports and preparing the data\n",
    "We load the data from `tf.keras.datasets`. Preprocessing consists of flattening and scaling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import libspn as spn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from libspn.examples.utils.dataiterator import DataIterator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "def scale(x):\n",
    "    return x / 255.\n",
    "\n",
    "def flatten(x):\n",
    "    return x.reshape(-1, np.prod(x.shape[1:]))\n",
    "\n",
    "def preprocess(x, y):\n",
    "    return scale(flatten(x)), np.expand_dims(y, axis=1)\n",
    "\n",
    "# Preprocess\n",
    "train_x, train_y = preprocess(train_x, train_y)\n",
    "test_x, test_y = preprocess(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the hyperparameters\n",
    "Some hyperparameters for the SPN. \n",
    "- `num_subsets` is used for the `DenseSPNGenerator`. This corresponds to the number of variable subsets joined by product nodes in the SPN.\n",
    "- `num_mixtures` is used for the `DenseSPNGenerator`. This corresponds to the number of sum nodes per scope.\n",
    "- `num_decomps` is used for the `DenseSPNGenerator`. This corresponds to the number of decompositions generated at each level of products from top-down.\n",
    "- `num_vars` corresponds to the number of input variables (the number of pixels in the case of MNIST).\n",
    "- `balanced` is used for the `DenseSPNGenerator`. If true, then the generated SPN will have balanced subsets and will consequently be a balanced tree.\n",
    "- `input_dist` is the input distribution (the first product/sum layer in the SPN). `spn.DenseSPNGenerator.InputDist.RAW` corresponds to raw indicators being joined (so first layer is a product layer). `spn.DenseSPNGenerator.InputDist.MIXTURE` would correspond to a sums on top of each indicator.\n",
    "- `num_leaf_components` is the number of contineuous components in the leaf distribution\n",
    "- `inference_type` determines the kind of forward inference where `spn.InferenceType.MARGINAL` corresponds to sum nodes marginalizing their inputs. `spn.InferenceType.MPE` would correspond to having max nodes instead.\n",
    "- `num_classes`, `batch_size` and `num_epochs` should be obvious:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of variable subsets that a product joins\n",
    "num_subsets = 2\n",
    "# Number of sums per scope\n",
    "num_mixtures = 4\n",
    "# Number of variables\n",
    "num_vars = train_x.shape[1]\n",
    "# Number of decompositions per product layer\n",
    "num_decomps = 1\n",
    "# Generate balanced subsets -> balanced tree\n",
    "balanced = True\n",
    "# Input distribution. Raw corresponds to first layer being product that \n",
    "# takes raw indicators\n",
    "input_dist = spn.DenseSPNGenerator.InputDist.RAW\n",
    "# Number of different values at leaf (binary here, so 2)\n",
    "num_leaf_components = 2\n",
    "# Initial value for path count accumulators\n",
    "initial_accum_value = 0.0\n",
    "# Inference type (can also be spn.InferenceType.MPE) where \n",
    "# sum nodes are turned into max nodes\n",
    "inference_type = spn.InferenceType.MARGINAL\n",
    "# Using unweighted log probabilities when determining winning child during hard EM\n",
    "use_unweighted = False\n",
    "# Sample the MPE pahts\n",
    "sample_winner = False\n",
    "# Sample probabilities\n",
    "sample_prob = None\n",
    "# Add one smoothing\n",
    "additive_smoothing = 1.0\n",
    "# L0 prior\n",
    "l0_prior_factor = 16 / 50000\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 10\n",
    "batch_size = 16\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the SPN\n",
    "Our SPN consists of Gaussian leafs, a dense SPN per class and a root node connecting the 10 class-wise sub-SPNs. We also add an indicator node to the root node to model the latent class variable. Finally, we generate `Weight` nodes for the full SPN by using `spn.generate_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [tensorflow:__getattr__] From /home/jos/spn/libspn/libspn/graph/node.py:40: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[WARNING] [tensorflow:__getattr__] From /home/jos/spn/libspn/libspn/graph/leaf/continuous_base.py:82: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "[WARNING] [tensorflow:__getattr__] From /home/jos/spn/libspn/libspn/graph/leaf/location_scale.py:55: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "[WARNING] [tensorflow:__getattr__] From /home/jos/spn/libspn/libspn/graph/leaf/location_scale.py:61: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "[WARNING] [tensorflow:__getattr__] From /home/jos/spn/libspn/libspn/graph/leaf/continuous_base.py:92: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "[WARNING] [spn.ConvProducts:generate_sparse_kernels] Number of channels exceeds total number of combinations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPN depth: 13\n",
      "Number of products layers: 5\n",
      "Number of sums layers: 5\n"
     ]
    }
   ],
   "source": [
    "from libspn.examples.convspn.architecture import wicker_convspn_two_non_overlapping\n",
    "\n",
    "# Reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Leaf nodes\n",
    "normal_leafs = spn.NormalLeaf(\n",
    "    trainable_scale=True,\n",
    "    trainable_loc=True,\n",
    "    num_components=num_leaf_components, \n",
    "    num_vars=num_vars)\n",
    "\n",
    "# Get convolutional SPN\n",
    "root, class_roots = wicker_convspn_two_non_overlapping(\n",
    "    normal_leafs, num_channels_prod=[32, 32, 64, 64, 64], num_channels_sums=[32, 32, 64, 64, 64])\n",
    "# conv_0 = spn.ConvProducts(normal_leafs, dilation_rate=1, kernel_size=2, padding=)\n",
    "\n",
    "# x = randomize = spn.BlockRandomDecompositions(normal_leafs, num_decomps=10)\n",
    "\n",
    "# num_alterations = int(np.ceil(np.log2(num_vars)))\n",
    "# for i in range(num_alterations - 1):\n",
    "#     layer_suffix = \"_{}\".format(i)\n",
    "#     x = spn.BlockPermuteProduct(x, num_factors=2, name=\"Products\" + layer_suffix)\n",
    "#     x = spn.BlockSum(x, num_sums_per_block=4, name=\"Sums\" + layer_suffix)\n",
    "\n",
    "# layer_suffix = \"_{}\".format(num_alterations - 1)\n",
    "# x = spn.BlockPermuteProduct(x, num_factors=2, name=\"Products\" + layer_suffix)\n",
    "# x = spn.BlockSum(x, num_sums_per_block=1, name=\"ClassRoots\")\n",
    "# x = spn.BlockMergeDecompositions(x, num_decomps=1)\n",
    "# root = spn.BlockRootSum(x, name=\"Root\")\n",
    "\n",
    "# Add a IndicatorLeaf node to the root as a latent class variable\n",
    "class_indicators = root.generate_latent_indicators()\n",
    "\n",
    "# Generate the weights for the SPN rooted at `root`\n",
    "spn.generate_weights(root)\n",
    "\n",
    "print(\"SPN depth: {}\".format(root.get_depth()))\n",
    "print(\"Number of products layers: {}\".format(root.get_num_nodes(node_type=spn.LocalSums)))\n",
    "print(\"Number of sums layers: {}\".format(root.get_num_nodes(node_type=spn.ConvProductsDepthwise)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the TensorFlow graph\n",
    "Now that we have defined the SPN graph we can declare the TensorFlow operations needed for training and evaluation. We use the `HardEMLearning` class to help us out. The `MPEState` class can be used to find the MPE state of any node in the graph. In this case we might be interested in generating images or finding the most likely class based on the evidence elsewhere. These correspond to finding the MPE state for `leaf_indicators` and `class_indicators` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [tensorflow:new_func] From /home/jos/spn/libspn/libspn/graph/leaf/continuous_base.py:175: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "[WARNING] [tensorflow:new_func] From /home/jos/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py:2618: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "[WARNING] [tensorflow:new_func] From /home/jos/spn/libspn/libspn/graph/op/base_sum.py:658: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n",
      "[WARNING] [tensorflow:__getattr__] From /home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py:89: The name tf.nn.conv2d_backprop_input is deprecated. Please use tf.nn.conv2d_transpose instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Op for getting the log probability of the root\n",
    "root_log_prob = root.get_log_value(inference_type=inference_type)\n",
    "\n",
    "# Helper for constructing EM learning ops\n",
    "em_learning = spn.HardEMLearning(\n",
    "    initial_accum_value=initial_accum_value, root=root, value_inference_type=inference_type,\n",
    "    sample_prob=sample_prob, sample_winner=sample_winner, use_unweighted=use_unweighted,\n",
    "    l0_prior_factor=l0_prior_factor, additive_smoothing=additive_smoothing)\n",
    "\n",
    "# Accumulate counts and update weights\n",
    "online_em_update_op = em_learning.accumulate_and_update_weights()\n",
    "\n",
    "# Op for initializing accumulators\n",
    "init_accumulators = em_learning.reset_accumulators()\n",
    "\n",
    "# MPE state generator\n",
    "mpe_state_generator = spn.MPEState()\n",
    "# Generate MPE state ops for leaf indicator and class indicator\n",
    "normal_leaf_mpe, class_indicator_mpe = mpe_state_generator.get_state(root, normal_leafs, class_indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display TF Graph\n",
    "Only works with Chrome browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spn.display_tf_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SPN\n",
    "Here we we train while monitoring the likelihood. Note that we train the SPN generatively, which means that it does not optimize for discriminating between digits. This is why we observe lower accuracies than when e.g. training a discriminative model such as an MLP with cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 625/625 [00:41<00:00, 15.02it/s, LLH=-888.99]\n",
      "Training:   0%|          | 0/3750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training test LLH = -889.02\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Conv2DCustomBackpropInputOp only supports NHWC.\n\t [[node TrueMPEPath/ConvProductsDepthwise_4/Conv2DBackpropInput (defined at /home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py:96) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node TrueMPEPath/ConvProductsDepthwise_4/Conv2DBackpropInput:\n TrueMPEPath/ConvProductsDepthwise_4/Shape (defined at /home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py:90)\t\n TrueMPEPath/ConvProductsDepthwise_4/ones (defined at /home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py:91)\t\n TrueMPEPath/ConvProductsDepthwise_4/Reshape_2 (defined at /home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py:72)\n\nOriginal stack trace for 'TrueMPEPath/ConvProductsDepthwise_4/Conv2DBackpropInput':\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jos/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/jos/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/jos/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/jos/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/jos/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jos/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/jos/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-d7ec4c591fe5>\", line 11, in <module>\n    online_em_update_op = em_learning.accumulate_and_update_weights()\n  File \"/home/jos/spn/libspn/libspn/learning/hard_em.py\", line 76, in accumulate_and_update_weights\n    accumulate_updates = self.accumulate_updates()\n  File \"/home/jos/spn/libspn/libspn/learning/hard_em.py\", line 83, in accumulate_updates\n    self._mpe_path.get_mpe_path(self._root)\n  File \"/home/jos/spn/libspn/libspn/inference/mpe_path.py\", line 80, in get_mpe_path\n    compute_graph_up_down(root, down_fun=down_fun, graph_input=graph_input)\n  File \"/home/jos/spn/libspn/libspn/graph/algorithms.py\", line 144, in compute_graph_up_down\n    down_values[child] = down_fun(child, parent_vals)\n  File \"/home/jos/spn/libspn/libspn/inference/mpe_path.py\", line 68, in down_fun\n    for i in node.inputs], **kwargs)\n  File \"/home/jos/spn/libspn/libspn/graph/op/conv_products.py\", line 359, in _compute_log_mpe_path\n    return self._compute_mpe_path_common(counts, *input_values)\n  File \"/home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py\", line 96, in _compute_mpe_path_common\n    data_format=\"NCHW\")\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 2077, in conv2d_backprop_input\n    explicit_paddings, data_format, dilations, name)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1407, in conv2d_backprop_input\n    name=name)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Conv2DCustomBackpropInputOp only supports NHWC.\n\t [[{{node TrueMPEPath/ConvProductsDepthwise_4/Conv2DBackpropInput}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-22e0393b7716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             batch_llh, _ = sess.run(\n\u001b[0;32m---> 27\u001b[0;31m                 [root_log_prob, online_em_update_op], fd(batch_x, batch_y))\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mlog_likelihoods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_llh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_llh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Conv2DCustomBackpropInputOp only supports NHWC.\n\t [[node TrueMPEPath/ConvProductsDepthwise_4/Conv2DBackpropInput (defined at /home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py:96) ]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node TrueMPEPath/ConvProductsDepthwise_4/Conv2DBackpropInput:\n TrueMPEPath/ConvProductsDepthwise_4/Shape (defined at /home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py:90)\t\n TrueMPEPath/ConvProductsDepthwise_4/ones (defined at /home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py:91)\t\n TrueMPEPath/ConvProductsDepthwise_4/Reshape_2 (defined at /home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py:72)\n\nOriginal stack trace for 'TrueMPEPath/ConvProductsDepthwise_4/Conv2DBackpropInput':\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/jos/.local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/jos/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/jos/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/jos/.local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/jos/.local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/jos/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/jos/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/jos/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-d7ec4c591fe5>\", line 11, in <module>\n    online_em_update_op = em_learning.accumulate_and_update_weights()\n  File \"/home/jos/spn/libspn/libspn/learning/hard_em.py\", line 76, in accumulate_and_update_weights\n    accumulate_updates = self.accumulate_updates()\n  File \"/home/jos/spn/libspn/libspn/learning/hard_em.py\", line 83, in accumulate_updates\n    self._mpe_path.get_mpe_path(self._root)\n  File \"/home/jos/spn/libspn/libspn/inference/mpe_path.py\", line 80, in get_mpe_path\n    compute_graph_up_down(root, down_fun=down_fun, graph_input=graph_input)\n  File \"/home/jos/spn/libspn/libspn/graph/algorithms.py\", line 144, in compute_graph_up_down\n    down_values[child] = down_fun(child, parent_vals)\n  File \"/home/jos/spn/libspn/libspn/inference/mpe_path.py\", line 68, in down_fun\n    for i in node.inputs], **kwargs)\n  File \"/home/jos/spn/libspn/libspn/graph/op/conv_products.py\", line 359, in _compute_log_mpe_path\n    return self._compute_mpe_path_common(counts, *input_values)\n  File \"/home/jos/spn/libspn/libspn/graph/op/conv_products_depthwise.py\", line 96, in _compute_mpe_path_common\n    data_format=\"NCHW\")\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 2077, in conv2d_backprop_input\n    explicit_paddings, data_format, dilations, name)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1407, in conv2d_backprop_input\n    name=name)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/home/jos/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# Set up some convenient iterators\n",
    "train_iterator = DataIterator([train_x, train_y], batch_size=batch_size)\n",
    "test_iterator = DataIterator([test_x, test_y], batch_size=batch_size)\n",
    "\n",
    "def fd(x, y):\n",
    "    return {normal_leafs: x, class_indicators: y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize things\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Do one run for test likelihoods\n",
    "    log_likelihoods = []\n",
    "    for batch_x, batch_y in test_iterator.iter_epoch(\"Testing\"):\n",
    "        batch_llh = sess.run(root_log_prob, fd(batch_x, batch_y))\n",
    "        log_likelihoods.extend(batch_llh)\n",
    "        test_iterator.display_progress(LLH=\"{:.2f}\".format(np.mean(batch_llh)))\n",
    "    mean_test_llh = np.mean(log_likelihoods)\n",
    "    \n",
    "    print(\"Before training test LLH = {:.2f}\".format(mean_test_llh))                              \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train\n",
    "        log_likelihoods = []\n",
    "        for batch_x, batch_y in train_iterator.iter_epoch(\"Training\"):\n",
    "            batch_llh, _ = sess.run(\n",
    "                [root_log_prob, online_em_update_op], fd(batch_x, batch_y))\n",
    "            log_likelihoods.extend(batch_llh)\n",
    "            train_iterator.display_progress(LLH=\"{:.2f}\".format(np.mean(batch_llh)))\n",
    "        mean_train_llh = np.mean(log_likelihoods)\n",
    "        \n",
    "        # Test\n",
    "        log_likelihoods, matches = [], []\n",
    "        for batch_x, batch_y in test_iterator.iter_epoch(\"Testing\"):\n",
    "            batch_llh, batch_class_mpe = sess.run([root_log_prob, class_indicator_mpe], fd(batch_x, -np.ones_like(batch_y, dtype=int)))\n",
    "            log_likelihoods.extend(batch_llh)\n",
    "            matches.extend(np.equal(batch_class_mpe, batch_y))\n",
    "            test_iterator.display_progress(LLH=\"{:.2f}\".format(np.mean(batch_llh)))\n",
    "        mean_test_llh = np.mean(log_likelihoods)\n",
    "        mean_test_acc = np.mean(matches)\n",
    "        \n",
    "        # Report\n",
    "        print(\"Epoch {}, train LLH = {:.2f}, test LLH = {:.2f}, test accuracy = {:.2f}\".format(\n",
    "            epoch, mean_train_llh, mean_test_llh, mean_test_acc))\n",
    "    \n",
    "    # Compute MPE state of all digits\n",
    "    per_class_mpe = sess.run(\n",
    "        normal_leaf_mpe, \n",
    "        fd(\n",
    "            -np.ones([num_classes, num_vars], dtype=int), \n",
    "            np.expand_dims(np.arange(num_classes, dtype=int), 1)\n",
    "        )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize MPE state per class\n",
    "We can visualize the MPE state computed at the end of the script above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in per_class_mpe:\n",
    "    _, ax = plt.subplots()\n",
    "    ax.imshow(sample.reshape(28, 28).astype(float), cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
